{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acba994a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T05:09:03.940786Z",
     "start_time": "2024-05-07T05:08:57.495678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import os, warnings, shutil, json, zipfile, random, math, cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet import ResNet152\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea0c9e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T05:09:03.944804Z",
     "start_time": "2024-05-07T05:09:03.941791Z"
    }
   },
   "outputs": [],
   "source": [
    "#resolution\n",
    "im_size = 244\n",
    "\n",
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc094e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T05:09:03.954688Z",
     "start_time": "2024-05-07T05:09:03.946816Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dir = 'train'\n",
    "test_dir = 'test'\n",
    "valid_dir = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14023385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T05:09:04.616967Z",
     "start_time": "2024-05-07T05:09:03.956617Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory train",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_load \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(train_dir, image_size \u001b[38;5;241m=\u001b[39m [im_size, im_size], interpolation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m test_load \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(test_dir, image_size \u001b[38;5;241m=\u001b[39m [im_size, im_size], interpolation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m valid_load \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(valid_dir, image_size \u001b[38;5;241m=\u001b[39m [im_size, im_size], interpolation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\image_dataset.py:210\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[1;32m--> 210\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mindex_directory(\n\u001b[0;32m    211\u001b[0m     directory,\n\u001b[0;32m    212\u001b[0m     labels,\n\u001b[0;32m    213\u001b[0m     formats\u001b[38;5;241m=\u001b[39mALLOWLIST_FORMATS,\n\u001b[0;32m    214\u001b[0m     class_names\u001b[38;5;241m=\u001b[39mclass_names,\n\u001b[0;32m    215\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m    216\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    217\u001b[0m     follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:542\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    541\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    544\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[1;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[0;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    778\u001b[0m ]\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find directory train"
     ]
    }
   ],
   "source": [
    "train_load = image_dataset_from_directory(train_dir, image_size = [im_size, im_size], interpolation = 'bilinear')\n",
    "test_load = image_dataset_from_directory(test_dir, image_size = [im_size, im_size], interpolation = 'bilinear')\n",
    "valid_load = image_dataset_from_directory(valid_dir, image_size = [im_size, im_size], interpolation = 'bilinear')\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train = (train_load.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE))\n",
    "test = (test_load.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE))\n",
    "valid = (valid_load.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0370208e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T05:11:17.613440Z",
     "start_time": "2024-05-07T05:11:17.608656Z"
    }
   },
   "outputs": [],
   "source": [
    "#some images raised an error so this finds all the corrupted images\n",
    "def check_image_readability(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    \n",
    "    errored_images = []\n",
    "    correct_images = []\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                img.load()\n",
    "            correct_images.append(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(file_path)\n",
    "            errored_images.append(file_path)\n",
    "            continue\n",
    "    \n",
    "    if errored_images:\n",
    "        print(\"There are errored images\")\n",
    "    else:\n",
    "        print(\"All images are readable.\")\n",
    "    return errored_images, correct_images\n",
    "\n",
    "#move legible files to new location\n",
    "def move_images(images_list, to_dir):\n",
    "    if not os.path.exists(to_dir):\n",
    "        os.mkdir(to_dir)\n",
    "    \n",
    "    for image_path in images_list:\n",
    "        image_name = os.path.basename(image_path)\n",
    "        destination_path = os.path.join(to_dir, image_name)\n",
    "        shutil.copy(image_path, destination_path)\n",
    "    \n",
    "    print(\"Images moved successfully to\", to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "\n",
    "vgg=VGG16(include_top=False,weights='imagenet',input_shape=(size,size,3))\n",
    "vgg.trainable=False\n",
    "\n",
    "res50 = ResNet50(include_top=False, weights='imagenet', input_shape=(size,size,3))\n",
    "res50.trainable=False\n",
    "\n",
    "res152 = ResNet152(include_top=False, weights='imagenet', input_shape=(size,size,3))\n",
    "res152.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the models and plot scores for training and validation\n",
    "def fitting(model, loss = 'sparse_categorical_crossentropy', metrics =['accuracy'], optimiser = 'adam', ep = 25):\n",
    "    \n",
    "    model.compile(optimiser, loss = loss, metrics = metrics)\n",
    "    history = model.fit(train, validation_data = valid, epochs = ep)\n",
    "    history_frame = pd.DataFrame(history.history)\n",
    "    history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "    history_frame.loc[:, [metrics[0], 'val_' + metrics[0]]].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrices of predictions and true labels\n",
    "def cf_matrix(model):\n",
    "    actual = np.concatenate([y for x, y in valid], axis=0)\n",
    "    pred = np.argmax(model.predict(valid), axis=-1)\n",
    "    cm = confusion_matrix(actual, pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
